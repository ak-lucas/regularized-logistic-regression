{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística com Descida de Gradiente Regularizada\n",
    "\n",
    "Este notebook foi criado para facilitar a realização dos experimentos do trabalho. Caso funcione bem, podemos adotar esta ferramenta também em trabalhos futuros.\n",
    "\n",
    "**Conteúdo do notebook: **\n",
    "\n",
    "- classe com a implementação da regressão logística com descida de gradiente regularizada;\n",
    "- main com exemplo de utilização da classe;\n",
    "- código para gerar os resultados e realizar os experimentos;\n",
    "- avaliação dos resultados;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classe da regressão logística com descida de gradiente regularizada:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Hyperparameters: \n",
    "#\t-Lambda: fator de regularização\n",
    "#\t-learning_rate: taxa de aprendizado\n",
    "#\t-epochs: número de iterações\n",
    "\n",
    "class RegularizedLogisticRegression():\n",
    "\tdef __init__(self):\n",
    "\t\tself.theta_n = []\n",
    "\t\tself.theta_0 = 0.\n",
    "\t\tself.loss = []\n",
    "\n",
    "\tdef sigmoid(self, x):\n",
    "\t\treturn (1/(1+np.exp(-x)))\n",
    "\n",
    "\t#inicializa os pesos aleatoriamente com amostras da distribuição normal\n",
    "\tdef init_weights(self, dim):\n",
    "\t\treturn np.random.randn(dim).reshape(dim,1)\n",
    "\t\t#return np.ones(dim).reshape(dim,1)\n",
    "\n",
    "\t#função de custo: cross-entropy\n",
    "\tdef loss_function(self, Y, sigmoid_z, Lambda, m):\n",
    "        # VERIFICAR SE A PARTE DA REGULARIZAÇÃO ESTÁ CERTA\n",
    "\t\tloss = -np.sum(np.multiply(Y,np.log(sigmoid_z)) + np.multiply(1-Y,np.log(1-sigmoid_z)))/m + np.multiply(np.sum(np.power(self.theta_n,2)), Lambda)/m\n",
    "\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef prints(self, epoch):\n",
    "\t\tprint(\"--epoca %s: \" % epoch)\n",
    "\t\tprint(\"loss: \", self.loss[epoch])\n",
    "\t\tprint(\"theta: \", self.theta_0.reshape(theta[0].shape[0]), self.theta_n.reshape(theta[1].shape[0]))\n",
    "\n",
    "\tdef gradient_descent(self, epochs, X, Y, Lambda, learning_rate, m, print_results):\n",
    "\t\tfor i in range(epochs):\n",
    "\t\t\t#calcula Z\n",
    "\t\t\tZ = np.dot(self.theta_n.T, X) + self.theta_0\n",
    "\n",
    "\t\t\t#calcula gradientes\n",
    "\t\t\tsigmoid_z = self.sigmoid(Z)\t#função de ativação\n",
    "\n",
    "\t\t\tgZ = sigmoid_z - Y\n",
    "\t\t\t\n",
    "\t\t\tgTheta_n = np.dot(X, gZ.T)/m\n",
    "\t\t\tgTheta_0 = np.sum(gZ)/m\n",
    "\n",
    "\t\t\t#calcula função de custo\n",
    "\t\t\tloss = self.loss_function(Y, sigmoid_z, Lambda, m)\n",
    "\t\t\tself.loss.append(loss)\n",
    "\n",
    "\t\t\t#atualiza pesos\n",
    "\t\t\tself.theta_0 -= learning_rate*gTheta_0\n",
    "\t\t\tself.theta_n = self.theta_n*(1-(learning_rate*Lambda/m)) - learning_rate*gTheta_n\n",
    "\n",
    "\t\t\tif print_results:\n",
    "\t\t\t\tself.prints(i)\n",
    "\n",
    "\t\t#calcula função de custo final\n",
    "\t\tZ = np.dot(self.theta_n.T, X) + self.theta_0\n",
    "\t\tsigmoid_z = self.sigmoid(Z)\t#função de ativação\n",
    "\t\tloss = self.loss_function(Y, sigmoid_z, Lambda, m)\n",
    "\n",
    "\t\tself.loss.append(loss)\n",
    "\n",
    "\tdef fit(self, X, Y, epochs=3, learning_rate=0.01, Lambda=0.001, print_results=False):\n",
    "\t\t#dimensão dos dados\n",
    "\t\tm = X.shape[0]\n",
    "\t\tn = X.shape[1]\n",
    "\n",
    "\t\t#inicializa os pesos aleatoriamente\n",
    "\t\tself.theta_n = self.init_weights(n)\n",
    "\t\tself.theta_0 = self.init_weights(1)\n",
    "\t\t\n",
    "\t\tX = X.T\n",
    "\t\tY = Y.reshape(1,m)\n",
    "\n",
    "\t\t#verifica as dimensões\n",
    "\t\t#assert(self.theta_n.shape[0] == X.shape[0])\n",
    "\t\t\n",
    "\t\tself.gradient_descent(epochs, X, Y, Lambda, learning_rate, m, print_results)\n",
    "\n",
    "\t\treturn self\n",
    "\n",
    "\tdef accuracy_score(self, X, Y):\n",
    "\t\tm = X.shape[0]\n",
    "\t\tY_pred = self.predict(X)\n",
    "\t\t#número de exemplos menos o número de erros dividido pelo número de exemplos\n",
    "\t\taccuracy =  float(m - np.sum(np.logical_xor(Y_pred, Y)))/m\n",
    "\n",
    "\t\treturn accuracy\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\tX = X.T\n",
    "\n",
    "\t\t#verifica as dimensões antes de fazer o produto interno\n",
    "\t\t#assert(self.theta_n.shape[0] == X.shape[0])\n",
    "\n",
    "\t\tZ = np.dot(self.theta_n.T, X) + self.theta_0\n",
    "\t\tsigmoid_z = self.sigmoid(Z)\t#função de ativação\n",
    "\n",
    "\t\t#Z.shape == (1,m)\n",
    "\t\t#sigmoid_z.shape = (1,m) -> todas as predições estão neste array\n",
    "\n",
    "\t\t#verifica se cada predição é maior ou igual a 0.5 e atribui classe 0 ou 1\n",
    "\t\tY_predict = np.greater_equal(sigmoid_z, 0.5)\n",
    "\n",
    "\t\treturn Y_predict.astype(int).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**exemplo de uso**\n",
    "\n",
    "Neste exemplo é utilizada uma versão modificada do dataset iris do repositório UCI. Neste dataset uma classe foi excluída para que o dataset fosse transformado para ser utlizado em um problema de classificação binária."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aqui no notebook não precisa importar, mas pra executar com o interpretador precisa\n",
    "#from regressao_logistica_regularizado import RegularizedLogisticRegression\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "#PREPARAÇÃO DO DATASET\n",
    "X = []\n",
    "Y = []\n",
    "with open('iris_mod.csv', 'r') as f:\n",
    "\treader = csv.reader(f)\n",
    "\tfor r in reader:\n",
    "\t\tx = r[:-1]\n",
    "\t\tX.append([float(a) for a in x])\n",
    "\t\tY.append(int(r[-1]))\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "indices = np.arange(X.shape[0])\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "X = X[indices]\n",
    "Y = Y[indices]\n",
    "\n",
    "TRAIN_SIZE = int(.8 * X.shape[0])\n",
    "\n",
    "X_train = X[:TRAIN_SIZE]\n",
    "Y_train = Y[:TRAIN_SIZE]\n",
    "\n",
    "X_test = X[TRAIN_SIZE:]\n",
    "Y_test = Y[TRAIN_SIZE:]\n",
    "\n",
    "#REGRESSÃO LOGÍSTICA\n",
    "LR = RegularizedLogisticRegression()\n",
    "\n",
    "LR.fit(X_train,Y_train, epochs=30, learning_rate=0.08, Lambda=0.1, print_results=False)\n",
    "print(\"train accuracy: \" + str(LR.accuracy_score(X_train,Y_train)*100.0) + \"%\")\n",
    "\n",
    "Y_predict = LR.predict(X_test)\n",
    "\n",
    "print('test accuracy: ' + str(LR.accuracy_score(X_test,Y_test)*100.0) + \"%\")\n",
    "#print accuracy_score(Y_test,Y_predict)\t#sklearn accuracy\n",
    "\n",
    "print('final loss: ' + str(LR.loss[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
